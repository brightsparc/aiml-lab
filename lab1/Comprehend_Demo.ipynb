{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehend Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Copyright [2017]-[2018] Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n",
    "\n",
    "http://aws.amazon.com/apache2.0/\n",
    "\n",
    "or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "***\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "#### Identity and Acces Management\n",
    "\n",
    "The user or role that executes the commands must have permissions in AWS Identity and Access Management (IAM) to perform those actions. AWS provides a set of managed policies that help you get started quickly. For our example, you should apply the following managed policy to your user or role:\n",
    "\n",
    "    ComprehendReadOnly\n",
    "\n",
    "Be aware that we recommend you follow AWS IAM best practices for production implementations, which is out of scope for this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Download the [first sample](https://github.com/fivethirtyeight/russian-troll-tweets/) of the russian troll tweaks that were published by FiveThirtyEight story [Why Weâ€™re Sharing 3 Million Russian Troll Tweets](https://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nv https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "comprehend = boto3.client('comprehend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest file\n",
    "df = pd.read_csv('IRAhandle_tweets_1.csv')[['language','author','region','following','followers','content']]\n",
    "print(df.size)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a distribution of followers\n",
    "df[['following', 'followers']].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment \n",
    "\n",
    "Use comprehend to extract sentiment.  \n",
    "\n",
    "Comprehend supports a number of [languages](https://docs.aws.amazon.com/comprehend/latest/dg/how-languages.html), lets look at a distribution of languages in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_top10 = df['language'].value_counts()[:10]\n",
    "lang_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a bath a back of english results, and take a look at the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of 25 results\n",
    "max_results = 25\n",
    "contents = list(df[df['language'] == 'English']['content'].head(max_results).values)\n",
    "contents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get back the sentiment for these results and plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = comprehend.batch_detect_sentiment(TextList=contents, LanguageCode='en')\n",
    "sentiments = pd.DataFrame([r['SentimentScore'] for r in resp['ResultList']])\n",
    "print(sentiments.head())\n",
    "sentiments.plot() # TODO: Change colors for results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate\n",
    "\n",
    "Get back some italian results and translate into english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "translate = boto3.client('translate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = list(df[df['language'] == 'Italian']['content'].head(max_results).values)\n",
    "sample = contents[4] # Pick a random sample \n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = translate.translate_text(\n",
    "    Text=sample,\n",
    "    SourceLanguageCode='it',\n",
    "    TargetLanguageCode='en'\n",
    ")\n",
    "response['TranslatedText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities\n",
    "\n",
    "Get Entities extracted from this content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some entities back\n",
    "resp = comprehend.batch_detect_entities(TextList=list(contents), LanguageCode='en')\n",
    "entities = list(itertools.chain.from_iterable([r['Entities'] for r in resp['ResultList']]))\n",
    "df_entities = pd.DataFrame(entities)\n",
    "df_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top most common person references\n",
    "df_entities[df_entities['Type'] == 'PERSON']['Text'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the key phrases that are being discussed in these tweets\n",
    "resp = comprehend.batch_detect_key_phrases(TextList=list(contents), LanguageCode='en')\n",
    "df_phrases = pd.DataFrame([[p['Text'] for p in r['KeyPhrases']] for r in resp['ResultList']])\n",
    "df_phrases.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
