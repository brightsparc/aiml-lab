{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Concierge\n",
    "\n",
    "## Host pre-trained endpoint for MXNET model\n",
    "\n",
    "In this notebook we will download a pre-trained MXNET model and deploy endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install mxnet==1.3.1\n",
    "!pip -q install sagemaker==1.18.9.post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import mxnet as mx\n",
    "import sagemaker\n",
    "\n",
    "print('pyversion: {}, mxnet: {}, sagemaker: {}'.format(\n",
    "    platform.python_version(), mx.__version__, sagemaker.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model if haven't already created from previous notebook\n",
    "import os\n",
    "\n",
    "if not os.path.exists('model.tar.gz'):\n",
    "    !aws s3 cp s3://deeplens-virtual-concierge-model/mobilefacenet/model.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model into SageMaker\n",
    "\n",
    "Open a new sagemaker session and upload the model on to the default S3 bucket. We can use the ``sagemaker.Session.upload_data`` method to do this. We need the location of where we exported the model from MXNet and where in our default bucket we want to store the model(``/model``). The default S3 bucket can be found using the ``sagemaker.Session.default_bucket`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='virtual-concierge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``sagemaker.mxnet.model.MXNetModel`` to import the model into SageMaker that can be deployed. We need the location of the S3 bucket where we have the model, the role for authentication and the entry_point where the model defintion is stored (``predict.py``). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data=model_data, role=role, entry_point='predict.py',\n",
    "                             py_version='py3', framework_version='1.3.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint. We can use the ``sagemaker.mxnet.model.MXNetModel.deploy`` method to do this. Unless you have created or prefer other instances, we recommend using 1 ``'ml.c4.xlarge'`` instance for this training. These are supplied as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an inference request\n",
    "\n",
    "Now that our Endpoint is deployed and we have a ``predictor`` object which we can call for inference.\n",
    "\n",
    "We wiill pass a single batch of an aligned image as a numpy array in the shape the model expects, setting `content_type` and `serializer` to convert into bytes.  The `predict.py` endpoint includes overides for `model_fn` to load fully connected layer and `transform_fn` to [transform](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html?highlight=input_fn#using-input-fn-predict-fn-and-output-fn) to load numpy input and return normalized emeddings as json.\n",
    "\n",
    "The SageMaker MXNet containers are [open source](https://github.com/aws/sagemaker-containers) if you needed more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send requests\n",
    "\n",
    "Download a sample picture, detect the first face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read image from s3\n",
    "image = {\n",
    "    'S3Object': {\n",
    "        'Bucket': 'aiml-lab-sagemaker',\n",
    "        'Name': 'politicians/politicians2.jpg'\n",
    "    }\n",
    "}\n",
    "image_object = s3.Object(image['S3Object']['Bucket'] , image['S3Object']['Name'])\n",
    "payload = image_object.get()['Body'].read()\n",
    "\n",
    "rekognition = boto3.client('rekognition')\n",
    "    \n",
    "# Call rekognition to get bbox\n",
    "ret = rekognition.detect_faces(\n",
    "    Image={\n",
    "        'Bytes': payload\n",
    "    },\n",
    "    Attributes=['DEFAULT'],\n",
    ")\n",
    "\n",
    "print(ret['FaceDetails'][0]['BoundingBox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the image at the bounding box and resize for input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def crop_image(payload, bbox, image_size=(112, 112)):\n",
    "    # Load image and convert to RGB space\n",
    "    img = cv2.imdecode(np.frombuffer(payload, np.uint8), cv2.IMREAD_COLOR) \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Crop relative to image size\n",
    "    if bbox != None:\n",
    "        height, width, _ = img.shape\n",
    "        x1 = int(bbox['Left'] * width)\n",
    "        y1 = int(bbox['Top'] * height)\n",
    "        x2 = int(bbox['Left'] * width + bbox['Width'] * width)\n",
    "        y2 = int(bbox['Top'] * height + bbox['Height']  * height)\n",
    "        img = img[y1:y2,x1:x2,:]\n",
    "    # Resize\n",
    "    return cv2.resize(img, (image_size[1], image_size[0]))    \n",
    "\n",
    "image = crop_image(payload, ret['FaceDetails'][0]['BoundingBox'])\n",
    "    \n",
    "# Show the last image with size\n",
    "plt.imshow(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose the image data into a numpy array the model expects and save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = np.rollaxis(image, axis=2, start=0)[np.newaxis, :]\n",
    "np.save('input.npy', model_input)\n",
    "model_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the payload to the endpoint, and output the face embedding response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    import io\n",
    "    import numpy as np\n",
    "    \n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "model_input = np.load('input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set the content-type to numpy \n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/x-npy'\n",
    "predictor.serializer = numpy_bytes_serializer\n",
    "response = predictor.predict(model_input)\n",
    "predict_data = np.array(response)\n",
    "print(np.array(response)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Call the predictor with just the image payload (offloading the rekognition bbox to endpoint)\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/x-image'\n",
    "predictor.serializer = None\n",
    "response = predictor.predict(payload)\n",
    "predict_data = np.array(response)\n",
    "print(np.array(response)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Local\n",
    "\n",
    "Extract the model locally and call `model_fn` from `predict.py` to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from predict import model_fn, transform_fn\n",
    "\n",
    "if not os.path.exists('model'):\n",
    "    !mkdir -p model\n",
    "    !tar xvzf model.tar.gz -C ./model    \n",
    "\n",
    "model = model_fn('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `transform_fn` from `predict.py` to perform inference on test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "        \n",
    "model_input = np.load('input.npy')\n",
    "data = numpy_bytes_serializer(model_input)\n",
    "embedding, content_type = transform_fn(model, data, 'application/x-npy', 'application/json')\n",
    "local_data = np.array(json.loads(embedding))\n",
    "print(local_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predict_data - local_data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Tear down the sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "os.remove('input.npy')\n",
    "os.remove('model.tar.gz')\n",
    "shutil.rmtree('model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
