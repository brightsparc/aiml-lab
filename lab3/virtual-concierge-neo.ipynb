{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Concierge\n",
    "\n",
    "## Compile Pretrained MXNET model with NEO\n",
    "\n",
    "In this notebook we will download a pre-trained MXNET model and compile for `ml_m4` and `deeplens` targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained model if haven't already created from previous notebook\n",
    "import os\n",
    "\n",
    "if not os.path.exists('model.tar.gz'):\n",
    "    !aws s3 cp s3://deeplens-virtual-concierge-model/mobilefacenet/model.tar.gz ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Neo Compilation API\n",
    "\n",
    "We then forward the model artifact to Neo Compilation API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-423079281568\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "compilation_job_name = name_from_base('virtual-concierge')\n",
    "\n",
    "model_key = '{}/model/model.tar.gz'.format(compilation_job_name)\n",
    "model_path = 's3://{}/{}'.format(bucket, model_key)\n",
    "boto3.resource('s3').Bucket(bucket).upload_file('model.tar.gz', model_key)\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "data_shape = '{\"data\":[1,3,112,112]}'\n",
    "target_device = 'ml_m4'\n",
    "framework = 'MXNET'\n",
    "framework_version = '1.2'\n",
    "compiled_model_path = 's3://{}/{}/output'.format(bucket, compilation_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CompilationJobArn': 'arn:aws:sagemaker:us-east-1:423079281568:compilation-job/virtual-concierge-2019-04-03-00-00-36-900', 'ResponseMetadata': {'RequestId': 'f397848a-d9b1-427e-b14e-f022b83e906b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f397848a-d9b1-427e-b14e-f022b83e906b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '122', 'date': 'Wed, 03 Apr 2019 00:00:36 GMT'}, 'RetryAttempts': 0}}\n",
      "Compiling ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "response = sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': model_path,\n",
    "        'DataInputConfig': data_shape,\n",
    "        'Framework': framework\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': compiled_model_path,\n",
    "        'TargetDevice': target_device\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 300\n",
    "    }\n",
    ")\n",
    "print(response)\n",
    "\n",
    "# Poll every 30 sec\n",
    "while True:\n",
    "    response = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)\n",
    "    if response['CompilationJobStatus'] == 'COMPLETED':\n",
    "        break\n",
    "    elif response['CompilationJobStatus'] == 'FAILED':\n",
    "        raise RuntimeError('Compilation failed')\n",
    "    print('Compiling ...')\n",
    "    time.sleep(30)\n",
    "print('Done!')\n",
    "\n",
    "# Extract compiled model artifact\n",
    "compiled_model_path = response['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create prediction endpoint\n",
    "\n",
    "To create a prediction endpoint, we first specify two additional functions, to be used with Neo Deep Learning Runtime:\n",
    "\n",
    "* `neo_preprocess(payload, content_type)`: Function that takes in the payload and Content-Type of each incoming request and returns a NumPy array. Here, the payload is byte-encoded NumPy array, so the function simply decodes the bytes to obtain the NumPy array.\n",
    "* `neo_postprocess(result)`: Function that takes the prediction results produced by Deep Learining Runtime and returns the response body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir, prefered_batch_size=\u001b[34m1\u001b[39;49;00m, image_size=(\u001b[34m112\u001b[39;49;00m,\u001b[34m112\u001b[39;49;00m)):\r\n",
      "    \u001b[33m\"\"\"Function responsible for loading the model.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        model_dir (str): The directory where model files are stored\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        mxnet.mod.Module: the loaded model.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking model load\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)    \r\n",
      "    \r\n",
      "    data_shapes = [(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, (prefered_batch_size, \u001b[34m3\u001b[39;49;00m, image_size[\u001b[34m0\u001b[39;49;00m], image_size[\u001b[34m1\u001b[39;49;00m]))]\r\n",
      "\r\n",
      "    sym, args, aux = mx.model.load_checkpoint(os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "    ctx = mx.cpu()\r\n",
      "                   \r\n",
      "    model = mx.mod.Module(symbol=sym, context=ctx, label_names=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    model.bind(for_training=\u001b[36mFalse\u001b[39;49;00m, data_shapes=data_shapes)\r\n",
      "    model.set_params(args, aux, allow_missing=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# DEBUG: Print out the model summary to see its the same size\u001b[39;49;00m\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(mx.viz.print_summary(model.symbol))\r\n",
      "    \u001b[37m# print(model.get_params())\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_fn\u001b[39;49;00m(model, request_body, request_content_type, accept_type):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    Transform a request using the Gluon model. Called once per request.\u001b[39;49;00m\r\n",
      "\u001b[33m    :param model: The model.\u001b[39;49;00m\r\n",
      "\u001b[33m    :param request_body: The request payload.\u001b[39;49;00m\r\n",
      "\u001b[33m    :param request_content_type: The request content type.\u001b[39;49;00m\r\n",
      "\u001b[33m    :param accept_type: The (desired) response content type.\u001b[39;49;00m\r\n",
      "\u001b[33m    :return: response payload and content type.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking transform Shape: {}, Content Type: {}, Accept: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\r\n",
      "        model.data_shapes[\u001b[34m0\u001b[39;49;00m][\u001b[34m1\u001b[39;49;00m], request_content_type, accept_type))\r\n",
      "    \r\n",
      "    array = neo_preprocess(request_body, request_content_type)\r\n",
      "    \r\n",
      "    \u001b[37m#logging.debug('Model input: {}'.format(array))\u001b[39;49;00m\r\n",
      "    logging.debug(\u001b[33m'\u001b[39;49;00m\u001b[33mModel input: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(array))\r\n",
      "\r\n",
      "    data = mx.nd.array(array)\r\n",
      "    db = mx.io.DataBatch(data=(data,))\r\n",
      "    model.forward(db, is_train=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    output = model.get_outputs()[\u001b[34m0\u001b[39;49;00m].asnumpy()\r\n",
      "\r\n",
      "    logging.debug(\u001b[33m'\u001b[39;49;00m\u001b[33mModel output: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output))\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m accept_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m neo_postprocess(output)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mAccept header must be application/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m### NOTE: This function is used within lambda layer\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_load\u001b[39;49;00m(model_path=\u001b[33m'\u001b[39;49;00m\u001b[33mcompiled\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, device=\u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdlr\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DLRModel \u001b[37m# Import the relative dlr library\u001b[39;49;00m\r\n",
      "    \r\n",
      "    model_path = os.path.join(os.path.dirname(os.path.abspath(\u001b[31m__file__\u001b[39;49;00m)), model_path)    \r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading model: {} on: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model_path, device))\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m DLRModel(model_path, device)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_inference\u001b[39;49;00m(model, data):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking inference for model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    input_data = {\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: data}\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.run(input_data)[\u001b[34m0\u001b[39;49;00m]\r\n",
      "\r\n",
      "\u001b[37m### NOTE: this function cannot use MXNet\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_preprocess\u001b[39;49;00m(payload, content_type, bbox=\u001b[36mNone\u001b[39;49;00m, image_size=(\u001b[34m112\u001b[39;49;00m, \u001b[34m112\u001b[39;49;00m)):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined pre-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        f = io.BytesIO(payload)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m np.load(f)\r\n",
      "    \u001b[34melif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        \u001b[37m# Only load PIL if required to transform bytes\u001b[39;49;00m\r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mPIL.Image\u001b[39;49;00m\r\n",
      "        f = io.BytesIO(payload)\r\n",
      "        \u001b[37m# Load image and convert to RGB space\u001b[39;49;00m\r\n",
      "        image = PIL.Image.open(f).convert(\u001b[33m'\u001b[39;49;00m\u001b[33mRGB\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[37m# Crop relative to image size\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m bbox != \u001b[36mNone\u001b[39;49;00m:\r\n",
      "            width, height = image.size\r\n",
      "            x1 = \u001b[36mint\u001b[39;49;00m(bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mLeft\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] * width)\r\n",
      "            y1 = \u001b[36mint\u001b[39;49;00m(bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mTop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] * height)\r\n",
      "            x2 = \u001b[36mint\u001b[39;49;00m(bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mLeft\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] * width + bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mWidth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] * width)\r\n",
      "            y2 = \u001b[36mint\u001b[39;49;00m(bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mTop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] * height + bbox[\u001b[33m'\u001b[39;49;00m\u001b[33mHeight\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]  * height)\r\n",
      "            image = image.crop((x1, y1, x2, y2))\r\n",
      "        \u001b[37m# Resize\u001b[39;49;00m\r\n",
      "        image = image.resize(image_size)\r\n",
      "         \u001b[37m# Transpose\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m np.rollaxis(np.asarray(image), axis=\u001b[34m2\u001b[39;49;00m, start=\u001b[34m0\u001b[39;49;00m)[np.newaxis, :]\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mRuntimeError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mContent type must be application/x-image or application/x-npy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m### NOTE: this function cannot use MXNet\u001b[39;49;00m\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mneo_postprocess\u001b[39;49;00m(result):\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32ml2_normalize\u001b[39;49;00m(X):\r\n",
      "        norms = np.sqrt((X * X).sum(axis=\u001b[34m1\u001b[39;49;00m))\r\n",
      "        X /= norms[:, np.newaxis]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m X\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mInvoking user-defined post-processing function\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Return a normalize embedding\u001b[39;49;00m\r\n",
      "    embedding = l2_normalize(np.array(result)).flatten()\r\n",
      "    \r\n",
      "    response_body = json.dumps(embedding.tolist())\r\n",
      "    content_type = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m response_body, content_type\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Python script containing the two functions to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "source_key = '{}/source/sourcedir.tar.gz'.format(compilation_job_name)\n",
    "source_path = 's3://{}/{}'.format(bucket, source_key)\n",
    "\n",
    "with tarfile.open('sourcedir.tar.gz', 'w:gz') as f:\n",
    "    f.add('predict.py')\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).upload_file('sourcedir.tar.gz', source_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a SageMaker model record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelArn': 'arn:aws:sagemaker:us-east-1:423079281568:model/virtual-concierge-2019-04-03-00-01-08-702ml-m4', 'ResponseMetadata': {'RequestId': '9e3291f4-89bc-4dcf-b20c-2c7faa5dfeee', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9e3291f4-89bc-4dcf-b20c-2c7faa5dfeee', 'content-type': 'application/x-amz-json-1.1', 'content-length': '108', 'date': 'Wed, 03 Apr 2019 00:01:09 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import NEO_IMAGE_ACCOUNT\n",
    "from sagemaker.fw_utils import create_image_uri\n",
    "\n",
    "model_name = name_from_base('virtual-concierge') + target_device.replace('_', '-')\n",
    "\n",
    "image_uri = create_image_uri(region, 'neo-' + framework.lower(), target_device.replace('_', '.'),\n",
    "                             framework_version, py_version='py3', account=NEO_IMAGE_ACCOUNT[region])\n",
    "\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image_uri,\n",
    "        'ModelDataUrl': compiled_model_path,\n",
    "        'Environment': { 'SAGEMAKER_SUBMIT_DIRECTORY': source_path }\n",
    "    },\n",
    "    ExecutionRoleArn=role\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an Endpoint Configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:423079281568:endpoint-config/virtual-concierge-2019-04-03-00-01-08-702ml-m4', 'ResponseMetadata': {'RequestId': 'ecd563ba-2cc2-43b8-8066-7d503a7cb1a0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ecd563ba-2cc2-43b8-8066-7d503a7cb1a0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Wed, 03 Apr 2019 00:01:09 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "config_name = model_name\n",
    "\n",
    "response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default-variant-name',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.m4.xlarge',\n",
    "            'InitialVariantWeight': 1.0\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create an Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:423079281568:endpoint/virtual-concierge-2019-04-03-00-01-08-702ml-m4', 'ResponseMetadata': {'RequestId': '823dd7ff-09d1-418f-843d-21e6a72be2a7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '823dd7ff-09d1-418f-843d-21e6a72be2a7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '114', 'date': 'Wed, 03 Apr 2019 00:01:09 GMT'}, 'RetryAttempts': 0}}\n",
      "Creating endpoint ...\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = model_name\n",
    "\n",
    "response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=config_name,\n",
    ")\n",
    "print(response)\n",
    "\n",
    "print('Creating endpoint ...')\n",
    "sm_client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send requests\n",
    "\n",
    "Download a sample picture, detect the first face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import PIL.Image\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Read image from s3\n",
    "image = {\n",
    "    'S3Object': {\n",
    "        'Bucket': 'aiml-lab-sagemaker',\n",
    "        'Name': 'politicians/politicians2.jpg'\n",
    "    }\n",
    "}\n",
    "\n",
    "image_object = s3.Object(image['S3Object']['Bucket'] , image['S3Object']['Name'])\n",
    "payload = image_object.get()['Body'].read()\n",
    "\n",
    "rekognition = boto3.client('rekognition')\n",
    "    \n",
    "# Call rekognition to get bbox\n",
    "ret = rekognition.detect_faces(\n",
    "    Image={\n",
    "        'Bytes': payload\n",
    "    },\n",
    "    Attributes=['DEFAULT'],\n",
    ")\n",
    "\n",
    "print(ret['FaceDetails'][0]['BoundingBox'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop the image at the bounding box, resize and convert to bytes read for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(payload, bbox, image_size=(112, 112)):\n",
    "    f = io.BytesIO(payload)\n",
    "    # Load image and convert to RGB space\n",
    "    image = PIL.Image.open(f).convert('RGB')\n",
    "    # Crop relative to image size\n",
    "    if bbox != None:\n",
    "        width, height = image.size\n",
    "        x1 = int(bbox['Left'] * width)\n",
    "        y1 = int(bbox['Top'] * height)\n",
    "        x2 = int(bbox['Left'] * width + bbox['Width'] * width)\n",
    "        y2 = int(bbox['Top'] * height + bbox['Height']  * height)\n",
    "        image = image.crop((x1, y1, x2, y2))\n",
    "    # Resize\n",
    "    return image.resize(image_size)\n",
    "\n",
    "# Get a croped image from bytes\n",
    "image = crop_image(payload, ret['FaceDetails'][0]['BoundingBox'])\n",
    "\n",
    "# Convert the resized image to bytes\n",
    "imageBytes = io.BytesIO()\n",
    "image.save(imageBytes, format='PNG')\n",
    "payload = imageBytes.getvalue()\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the payload to the endpoint, and output the face embedding response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      ContentType='application/x-image',\n",
    "                                      Body=payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(json.loads(response['Body'].read().decode()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a saved numpy payload to end endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    import io\n",
    "    import numpy as np\n",
    "    \n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "model_input = np.load('input.npy')\n",
    "payload = numpy_bytes_serializer(model_input)\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                      ContentType='application/x-npy',\n",
    "                                      Body=payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(json.loads(response['Body'].read().decode()))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Tear down the Neo endpoint, configuration and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '24821821-0187-483f-aaea-875acd2141fe',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '24821821-0187-483f-aaea-875acd2141fe',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 01 Apr 2019 08:16:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'cff53739-4f1b-4398-922a-1d68b86a422e',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'cff53739-4f1b-4398-922a-1d68b86a422e',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 01 Apr 2019 08:16:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_endpoint_config(EndpointConfigName=config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'ae6e41a0-72e2-45c1-81ad-1a1e4781b315',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ae6e41a0-72e2-45c1-81ad-1a1e4781b315',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 01 Apr 2019 08:16:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
