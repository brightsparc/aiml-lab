{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load boto references\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create people database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input event wehich specifies the input images, inference endpoint and output people database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-882607831196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"InputBucket\": \"virtual-concierge-index-ap-southeast-2\", \"InputPrefix\": \"\", \"EndpointName\": \"sagemaker-mxnet-2019-04-28-00-04-50-699\", \"OutputBucket\": \"sagemaker-us-east-1-882607831196\", \"OutputKey\": \"virtual-concierge/people.npz\"}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "event = {\n",
    "    'InputBucket': 'virtual-concierge-index-ap-southeast-2',\n",
    "    'InputPrefix': '',\n",
    "    'EndpointName': 'sagemaker-mxnet-2019-04-28-00-04-50-699' # TODO: Replace this with your endpoint\n",
    "}\n",
    "\n",
    "event['OutputBucket'] = sagemaker_session.default_bucket()\n",
    "event['OutputKey'] = 'virtual-concierge/people.npz'\n",
    "\n",
    "json.dumps(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 rm s3://sagemaker-us-east-1-882607831196/virtual-concierge/people.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load existing vecs names and checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etag(event):\n",
    "    try:\n",
    "        people_head = s3_client.head_object(Bucket=event['OutputBucket'], Key=event['OutputKey'])\n",
    "        return people_head['ETag'].strip('\"')\n",
    "    except Exception as e:\n",
    "        print('Unable to get etag', e)\n",
    "        return ''\n",
    "\n",
    "def load_file(event):\n",
    "    try:\n",
    "        # Attempt to load people from s3\n",
    "        people_object = s3.Object(event['OutputBucket'], event['OutputKey'])\n",
    "        payload = people_object.get()['Body'].read()\n",
    "        f = io.BytesIO(payload)\n",
    "        people = np.load(f)\n",
    "        return people['vecs'].tolist(), people['names'].tolist(), people['keys'].tolist(), set(people['checksums'])\n",
    "    except Exception as e:\n",
    "        print('Initialize new file', e)\n",
    "        return [], [], [], set()\n",
    "        \n",
    "# Save the npz to temp file and get payload\n",
    "def save_file(event, vecs, names, keys, checksums):\n",
    "    from tempfile import TemporaryFile\n",
    "    outfile = TemporaryFile()\n",
    "    np.savez(outfile, vecs=vecs, names=names, keys=keys, checksums=list(checksums))\n",
    "    outfile.seek(0)\n",
    "    payload = outfile.read()\n",
    "    resp = s3_client.put_object(Bucket=event['OutputBucket'], Key=event['OutputKey'], Body=payload)\n",
    "    return resp['ETag'].strip('\"')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the existing file from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded count: 63\n",
      "CPU times: user 14.3 ms, sys: 4.62 ms, total: 18.9 ms\n",
      "Wall time: 47.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vecs, names, keys, checksums = load_file(event)\n",
    "print('loaded count: {}'.format(len(checksums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query s3 for keys that have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting contents batch size: 10, limit: 100\n",
      "Added 0 contents, truncated: False\n",
      "CPU times: user 55 ms, sys: 2.57 ms, total: 57.6 ms\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def get_new_contents(event, checksums, batch_size=10, batch_limit=100):    \n",
    "    print('getting contents batch size: {}, limit: {}'.format(batch_size, batch_limit))\n",
    "    contents = []\n",
    "    is_truncated = False\n",
    "    \n",
    "    def filter_by_checksum(response, checksums):\n",
    "        return [(content['Key'], content['ETag'].strip('\"')) for content in response['Contents']\n",
    "                if content['Size'] > 0 and not content['ETag'].strip('\"') in checksums]\n",
    "    \n",
    "    # Get the first response\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=event['InputBucket'],\n",
    "        Prefix=event.get('InputPrefix'),\n",
    "        MaxKeys=batch_size\n",
    "    )\n",
    "    contents += filter_by_checksum(response, checksums)\n",
    "\n",
    "    # Get remaining response\n",
    "    while response['IsTruncated']:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            ContinuationToken=response['NextContinuationToken'],\n",
    "            Bucket=event['InputBucket'],\n",
    "            Prefix=event.get('InputPrefix'),\n",
    "            MaxKeys=batch_size\n",
    "        )\n",
    "        contents += filter_by_checksum(response, checksums)\n",
    "        if len(contents) > batch_limit:\n",
    "            print('Reached limit: {}'.format(len(contents)))\n",
    "            is_truncated = True\n",
    "            break\n",
    "    \n",
    "    return contents, is_truncated\n",
    "\n",
    "contents, is_truncated = get_new_contents(event, checksums)\n",
    "print('Added {} contents, truncated: {}'.format(len(contents), is_truncated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading new images and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading contents: 0\n",
      "CPU times: user 226 µs, sys: 0 ns, total: 226 µs\n",
      "Wall time: 138 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def download_contents(event, contents, vecs, names, keys, checksums=None):\n",
    "    for (key, checksum) in contents:\n",
    "        if checksums and checksum in checksums:\n",
    "            print('Skip', checksum)\n",
    "            continue\n",
    "        # Get image object\n",
    "        image_object = s3.Object(event['InputBucket'], key)\n",
    "        # Get name from meta data or last part of filename\n",
    "        name = image_object.metadata.get('fullname') or key.split('/')[-1].split('.')[0]\n",
    "        # Call endpoint to crop boudning box and return vector \n",
    "        payload = image_object.get()['Body'].read()\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=event['EndpointName'],\n",
    "                                              ContentType='application/x-image',\n",
    "                                              Body=payload)\n",
    "        vec = json.loads(response['Body'].read().decode())\n",
    "        try:\n",
    "            # Attempt to replace key, or else, append to list\n",
    "            index = keys.index(key)\n",
    "            vecs[index] = vec\n",
    "            names[index] = name\n",
    "            print('Updated', index, name)\n",
    "        except ValueError:\n",
    "            print('Added', name)\n",
    "            vecs.append(vec)\n",
    "            names.append(name)\n",
    "            keys.append(key)\n",
    "        if checksums:\n",
    "            checksums.add(checksum)\n",
    "        \n",
    "print('downloading contents: {}'.format(len(contents)))\n",
    "download_contents(event, contents, vecs, names, keys, checksums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload updated file back to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_etag = ''\n",
    "\n",
    "if len(contents) > 0:\n",
    "    print('uploading file: {}/{}'.format(event['OutputBucket'], event['OutputKey']))\n",
    "    people_etag = save_file(event, vecs, names, keys, checksums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Added': 0, 'IsTruncated': False, 'Total': 63, 'Unique': 63, 'ETag': ''}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the new items added\n",
    "response = {\n",
    "    'Added': len(contents),\n",
    "    'IsTruncated': is_truncated,\n",
    "    'Total': len(keys),\n",
    "    'Unique': len(checksums),\n",
    "    'ETag': people_etag,\n",
    "}\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upadate Device Shadow with file to download\n",
    "\n",
    "Send a message to IoT shadow `thing_name` to inform it to download new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "client = boto3.client('iot-data')\n",
    "\n",
    "# TODO: Replace this with your thing name\n",
    "thing_name = 'deeplens_9pJ_x7I6QtO8FsjQfab2tQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send a message to IoT given a thing name to update their people model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the properties to send to shadow\n",
    "props = {\n",
    "    'people': {\n",
    "        'Etag': people_etag,\n",
    "        'Bucket': event['OutputBucket'],\n",
    "        'Key': event['OutputKey']\n",
    "    }\n",
    "}\n",
    "props\n",
    "\n",
    "shadow = {\n",
    "    'state': {\n",
    "        'desired' : props\n",
    "    }    \n",
    "}\n",
    "response = client.update_thing_shadow(\n",
    "    thingName=thing_name,\n",
    "    payload=json.dumps(shadow)\n",
    ")\n",
    "shadow = json.loads(response[\"payload\"].read())\n",
    "shadow['state']['desired']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Device Shadow delta, download and report back\n",
    "\n",
    "Get the shing shadow delta, and inspect for `people` data to download new model\n",
    "\n",
    "This code would typically be run at the edge in a greengrass lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "people_path = 'models/people.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def download_model(people_path, props):\n",
    "    try:\n",
    "        with open(people_path,'rb') as f:\n",
    "            people_etag = hashlib.md5(f.read()).hexdigest()\n",
    "            print('read etag', people_etag)\n",
    "    except:\n",
    "        people_etag = ''\n",
    "        \n",
    "    if people_etag == props['Etag']:\n",
    "        print('people unchanged')\n",
    "    else:\n",
    "        print('downloading to', people_path)\n",
    "        with open(people_path,'wb') as f:\n",
    "            response = s3_client.get_object(Bucket=props['Bucket'], Key=props['Key'])   \n",
    "            f.write(response['Body'].read())\n",
    "\n",
    "# Read the shadow, call download model if we have \n",
    "response = client.get_thing_shadow(thingName=thing_name)\n",
    "shadow = json.loads(response[\"payload\"].read())\n",
    "delta = shadow['state'].get('delta')\n",
    "print('shadow delta', delta)\n",
    "\n",
    "if delta and 'people' in delta:\n",
    "    download_model(people_path, shadow['state']['desired']['people'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the report value in shadow once we have downloaded the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the reported state to update\n",
    "shadow = {\n",
    "    'state': {\n",
    "        'reported': props\n",
    "    }    \n",
    "}\n",
    "response = client.update_thing_shadow(\n",
    "    thingName=thing_name,\n",
    "    payload=json.dumps(shadow)\n",
    ")\n",
    "shadow = json.loads(response[\"payload\"].read())\n",
    "shadow['state']['reported']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
