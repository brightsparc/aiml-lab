{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Load boto references\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "sm_runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create people database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input event wehich specifies the input images, inference endpoint and output people database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "event = {\n",
    "    'InputBucket': 'aiml-lab-sagemaker',\n",
    "    'InputPrefix': 'actors/',\n",
    "    'EndpointName': 'virtual-concierge-2019-04-25-10-30-39-241ml-m4' # TODO: Replace this with your endpoint\n",
    "}\n",
    "\n",
    "event['OutputBucket'] = sagemaker_session.default_bucket()\n",
    "event['OutputKey'] = 'virtual-concierge/people.npz'\n",
    "\n",
    "json.dumps(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load existing vecs names and checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etag(event):\n",
    "    try:\n",
    "        people_head = s3_client.head_object(Bucket=event['OutputBucket'], Key=event['OutputKey'])\n",
    "        return people_head['ETag'].strip('\"')\n",
    "    except Exception as e:\n",
    "        print('Unable to get etag', e)\n",
    "        return ''\n",
    "\n",
    "def load_file(event):\n",
    "    try:\n",
    "        # Attempt to load people from s3\n",
    "        people_object = s3.Object(event['OutputBucket'], event['OutputKey'])\n",
    "        payload = people_object.get()['Body'].read()\n",
    "        f = io.BytesIO(payload)\n",
    "        people = np.load(f)\n",
    "        return people['vecs'].tolist(), people['names'].tolist(), set(people['checksums'])\n",
    "    except Exception as e:\n",
    "        print('Initialize new file', e)\n",
    "        return [], [], set()\n",
    "        \n",
    "# Save the npz to temp file and get payload\n",
    "def save_file(event, vecs, names, checksums):\n",
    "    from tempfile import TemporaryFile\n",
    "    outfile = TemporaryFile()\n",
    "    np.savez(outfile, vecs=vecs, names=names, checksums=list(checksums))\n",
    "    outfile.seek(0)\n",
    "    payload = outfile.read()\n",
    "    resp = s3_client.put_object(Bucket=event['OutputBucket'], Key=event['OutputKey'], Body=payload)\n",
    "    return resp['ETag'].strip('\"')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the existing file from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vecs, names, checksums = load_file(event)\n",
    "print('loaded count: {}'.format(len(checksums)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query s3 for keys that have changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_new_keys(event, checksums):    \n",
    "    keys = []\n",
    "    batch_keys = 10\n",
    "    limit_keys = 100\n",
    "    is_truncated = False\n",
    "    \n",
    "    def filter_new_keys(response, checksums):\n",
    "        return [(content['Key'], content['ETag'].strip('\"')) for content in response['Contents']\n",
    "                if content['Size'] > 0 and not content['ETag'].strip('\"') in checksums]\n",
    "    \n",
    "    # Get the first response\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=event['InputBucket'],\n",
    "        Prefix=event.get('InputPrefix'),\n",
    "        Delimiter='/',\n",
    "        MaxKeys=batch_keys\n",
    "    )\n",
    "    keys += filter_new_keys(response, checksums)\n",
    "\n",
    "    # Get remaining response\n",
    "    while response['IsTruncated']:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            ContinuationToken=response['NextContinuationToken'],\n",
    "            Bucket=event['InputBucket'],\n",
    "            Prefix=event.get('InputPrefix'),\n",
    "            Delimiter='/',\n",
    "            MaxKeys=batch_keys\n",
    "        )\n",
    "        keys += filter_new_keys(response, checksums)\n",
    "        if len(keys) > limit_keys:\n",
    "            print('Reached limit: {}'.format(len(keys)))\n",
    "            is_truncated = True\n",
    "            break\n",
    "    \n",
    "    return keys, is_truncated\n",
    "\n",
    "keys, is_truncated = get_new_keys(event, checksums)\n",
    "print('Added {} keys, truncated: {}'.format(len(keys), is_truncated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading new images and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def download_keys(event, keys, vecs, names, checksums):\n",
    "    for (key, checksum) in keys:\n",
    "        if checksum in checksums:\n",
    "            print('Skip', checksum)\n",
    "            continue\n",
    "        # Get image object\n",
    "        image_object = s3.Object(event['InputBucket'], key)\n",
    "        # Get name from meta data or last part of filename\n",
    "        name = image_object.metadata.get('fullname') or key.split('/')[-1].split('.')[0]\n",
    "        # Call endpoint to crop boudning box and return vector \n",
    "        payload = image_object.get()['Body'].read()\n",
    "        response = sm_runtime.invoke_endpoint(EndpointName=event['EndpointName'],\n",
    "                                              ContentType='application/x-image',\n",
    "                                              Body=payload)\n",
    "        vec = json.loads(response['Body'].read().decode())\n",
    "        # Append vector to \n",
    "        vecs.append(vec)\n",
    "        names.append(name)\n",
    "        checksums.add(checksum)\n",
    "        print('Added', name)\n",
    "        \n",
    "print('downloading keys: {}'.format(len(keys)))\n",
    "download_keys(event, keys, vecs, names, checksums)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload updated file back to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(keys) > 0:   \n",
    "    print('uploading file: {}/{}'.format(event['OutputBucket'], event['OutputKey']))\n",
    "    people_etag = save_file(event, vecs, names, checksums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the new items added\n",
    "response = {    \n",
    "    'Added': len(keys),\n",
    "    'IsTruncated': is_truncated,\n",
    "    'Total': len(names),\n",
    "    'ETag': people_etag,\n",
    "}\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
