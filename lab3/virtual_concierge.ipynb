{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Concierge \n",
    "\n",
    "## Face Recognition Project with MXNet\n",
    "\n",
    "***\n",
    "Copyright [2017]-[2018] Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n",
    "\n",
    "http://aws.amazon.com/apache2.0/\n",
    "\n",
    "or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "#### Python package dependencies\n",
    "\n",
    "The following packages need to be installed before proceeding:\n",
    "\n",
    "* Boto3 - `pip install boto3`\n",
    "* MXNet - `pip install mxnet`\n",
    "* numpy - `pip install numpy`\n",
    "* OpenCV - `pip install opencv-python`\n",
    "* Graphviz - `pip install graphviz`\n",
    "* Matplotlib - `pip install matplotlib`\n",
    "* Seaborn - `pip install seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Verify that all dependencies are installed using the cell below. Continue if no errors encountered, warnings can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import boto3\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model\n",
    "\n",
    "`get_model()` : Loads MXNet symbols and params, defines model using symbol file and binds parameters to the model using params file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ctx, image_size, model_str, layer):\n",
    "    _vec = model_str.split(',')\n",
    "    assert len(_vec)==2\n",
    "    prefix = _vec[0]\n",
    "    epoch = int(_vec[1])\n",
    "    print('loading',prefix, epoch)\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    all_layers = sym.get_internals()\n",
    "    sym = all_layers[layer+'_output']\n",
    "    model = mx.mod.Module(symbol=sym, context=ctx, label_names = None)\n",
    "    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])\n",
    "    model.set_params(arg_params, aux_params)\n",
    "    return model, sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images\n",
    "\n",
    "In order to input only face pixels into the network, all input images are passed through a pretrained face detection and alignment model as described above. The output of this model are landmark points and a bounding box corresponding to the face in the image. Using this output, the image is processed using affine transforms to generate the aligned face images which are input to the network. The functions performing this is defined below.\n",
    "\n",
    "`get_input()` : Returns aligned face to the bbox and margin, and [rotation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html)\n",
    "\n",
    "`show_input()` : Shows the image after transposing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(img, image_size, bbox=None, margin=0, rotate=0):\n",
    "    if bbox is None:\n",
    "        det = np.zeros(4, dtype=np.int32)\n",
    "        det[0] = int(img.shape[1]*0.0625)\n",
    "        det[1] = int(img.shape[0]*0.0625)\n",
    "        det[2] = img.shape[1] - det[0]\n",
    "        det[3] = img.shape[0] - det[1]\n",
    "    else:\n",
    "        det = bbox\n",
    "    # Crop\n",
    "    bb = np.zeros(4, dtype=np.int32)\n",
    "    bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "    bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "    bb[2] = np.minimum(det[2]+margin/2, img.shape[1])\n",
    "    bb[3] = np.minimum(det[3]+margin/2, img.shape[0])\n",
    "    img = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "    # Rotate if required\n",
    "    if 0 < rotate and rotate < 360:\n",
    "        rows,cols,_ = img.shape\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),360-rotate,1)\n",
    "        img = cv2.warpAffine(img,M,(cols,rows))\n",
    "    # Resize and transform\n",
    "    img = cv2.resize(img, (image_size[1], image_size[0]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aligned = np.transpose(img, (2,0,1))\n",
    "    return aligned\n",
    "\n",
    "def show_input(aligned):\n",
    "    plt.imshow(np.transpose(aligned,(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features\n",
    "\n",
    "`l2_normalize()`: Performs row normalization on the vector\n",
    "\n",
    "`get_feature()` : Performs forward pass on the data aligned using model and returns the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(X):\n",
    "    norms = np.sqrt((X * X).sum(axis=1))\n",
    "    X /= norms[:, np.newaxis]\n",
    "    return X\n",
    "\n",
    "def get_feature(model, aligned):\n",
    "    input_blob = np.expand_dims(aligned, axis=0)\n",
    "    data = mx.nd.array(input_blob)\n",
    "    db = mx.io.DataBatch(data=(data,))\n",
    "    model.forward(db, is_train=False)\n",
    "    embedding = model.get_outputs()[0].asnumpy()\n",
    "    embedding = l2_normalize(embedding).flatten()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model\n",
    "\n",
    "Load the pre-trained mobilenet mobile, setting the context to cpu and visualize the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_size = (112,112)\n",
    "model_name = './models/mobilenet1,0'\n",
    "model, sym = get_model(mx.cpu(), image_size, model_name, 'fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.viz.plot_network(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulate\n",
    "\n",
    "Download sample image, and extract face coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aiml-lab-sagemaker/politicians/politicians2.jpg tmp/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "def get_bboxes(img, margin=0):\n",
    "    # Detect faces\n",
    "    ret, buf = cv2.imencode('.jpg', img)\n",
    "    ret = rekognition.detect_faces(\n",
    "        Image={\n",
    "            'Bytes': buf.tobytes()\n",
    "        },\n",
    "        Attributes=['DEFAULT'],\n",
    "    )\n",
    "    # Return the bounding boxes for each face\n",
    "    height, width, _ = img.shape\n",
    "    bboxes = []\n",
    "    for face in ret['FaceDetails']:\n",
    "        box = face['BoundingBox']\n",
    "        x1 = int(box['Left'] * width)\n",
    "        y1 = int(box['Top'] * height)\n",
    "        x2 = int(box['Left'] * width + box['Width'] * width)\n",
    "        y2 = int(box['Top'] * height + box['Height']  * height)\n",
    "        bboxes.append((x1, y1, x2, y2))\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the coordinates, get a the aligned image, and draw the rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the image, and get bboxes\n",
    "img = cv2.imread('tmp/image')\n",
    "boxes = get_bboxes(img)\n",
    "\n",
    "# blue, green, red, grey\n",
    "colors = ((220,220,220),(242,168,73),(76,182,252),(52,194,123))\n",
    "\n",
    "img_aligned = []\n",
    "for col, bbox in enumerate(boxes): \n",
    "    aligned = get_input(img, image_size, bbox)\n",
    "    img_aligned.append(aligned)\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), colors[col%4], 3)\n",
    "    \n",
    "# Plot the figure in it's original rotation\n",
    "plt.figure(figsize=(10,10))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the aligned image\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i, aligned in enumerate(img_aligned):\n",
    "    a = fig.add_subplot(1, len(img_aligned), i+1)\n",
    "    a.set_title('Image {}'.format(i))\n",
    "    show_input(aligned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embedding\n",
    "\n",
    "Pass each face through the network sequentially to generate embedding vectors for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vecs = np.array([get_feature(model, aligned) for aligned in img_aligned])\n",
    "print(img_vecs.shape)\n",
    "img_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity\n",
    "\n",
    "Calculate the cosine similarity between the embedding vectors to see how similar they are to each out. \n",
    "\n",
    "Similarity values in [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.dot(img_vecs, img_vecs.T)\n",
    "np.fill_diagonal(sims, 0)\n",
    "sns.heatmap(sims, annot=True, fmt=\".03f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Dataset\n",
    "\n",
    "Download a the politician dataset, and vectories the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp/images\n",
    "!aws s3 sync s3://aiml-lab-sagemaker/actors/ tmp/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_dir = 'tmp/images'\n",
    "names = []\n",
    "vecs = []\n",
    "\n",
    "for file in os.listdir(image_dir):\n",
    "    name = file.split('.')[0]\n",
    "    img = cv2.imread(os.path.join(image_dir, file))\n",
    "    bboxes = get_bboxes(img)\n",
    "    bbox = bboxes[0]\n",
    "    print(name, bbox)\n",
    "    aligned = get_input(img, image_size, bbox)\n",
    "    vec = get_feature(model, aligned)   \n",
    "    names.append(name)\n",
    "    vecs.append(vec)\n",
    "    \n",
    "vecs = np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectors back to a file with the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('people.npz', names=names, vecs=vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distribution\n",
    "\n",
    "Compare the vectors of all the politications to input image, plot the distribution and outliner for match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_vecs[0]\n",
    "\n",
    "# calculate cosine similarity and relative zscores\n",
    "sims = np.dot(vecs, img)\n",
    "zscores = stats.zscore(sims)\n",
    "\n",
    "# plot series and print score and name\n",
    "sns.set(color_codes=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.distplot(zscores, bins=50, kde=False, rug=True)\n",
    "ax.set(xlabel='zscore', ylabel='number of people')\n",
    "plt.title('zscore distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the name of the highest similarity based on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import erf, sqrt\n",
    "def phi(x):\n",
    "    #'Cumulative distribution function for the standard normal distribution'\n",
    "    return (1.0 + erf(x / sqrt(2.0))) / 2.0\n",
    "\n",
    "idx = sims.argmax()\n",
    "print('sim: {}, zscore: {}, prob: {}, name: {}'.format(sims[idx], zscores[idx], phi(zscores[idx]), names[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "\n",
    "### Export the model from mxnet\n",
    "\n",
    "In order to set up hosting, we have to [import the model from training to hosting](https://aws.amazon.com/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/). We will begin by exporting the model from MXNet and saving it down. The exported model has to be converted into a form that is readable by ``sagemaker.mxnet.model.MXNetModel``. The following code describes exporting the model in a form that does the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.mkdir('model')\n",
    "\n",
    "model.save_checkpoint('model/model', 0000)\n",
    "with open ('model/model-shapes.json', \"w\") as shapes:\n",
    "    json.dump([{\"shape\": model.data_shapes[0][1], \"name\": \"data\"}], shapes)\n",
    "\n",
    "import tarfile\n",
    "\n",
    "def flatten(tarinfo):\n",
    "    tarinfo.name = os.path.basename(tarinfo.name)\n",
    "    return tarinfo\n",
    "    \n",
    "tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n",
    "tar.add(\"model\", filter=flatten)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above piece of code essentially hacks the MXNet model export into a sagemaker-readable model export. Study the exported model files if you want to organize your exports in the same fashion as well. Alternatively, you can load the model on MXNet itself and load the sagemaker model as you normally would. Refer [here](https://github.com/aws/sagemaker-python-sdk#model-loading) for details on how to load MXNet models.\n",
    "\n",
    "### Import model into SageMaker\n",
    "\n",
    "Open a new sagemaker session and upload the model on to the default S3 bucket. We can use the ``sagemaker.Session.upload_data`` method to do this. We need the location of where we exported the model from MXNet and where in our default bucket we want to store the model(``/model``). The default S3 bucket can be found using the ``sagemaker.Session.default_bucket`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='virtual-concierge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the ``sagemaker.mxnet.model.MXNetModel`` to import the model into SageMaker that can be deployed. We need the location of the S3 bucket where we have the model, the role for authentication and the entry_point where the model defintion is stored (``predict.py``). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data = model_data, role = role, entry_point = 'predict.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Now the model is ready to be deployed at a SageMaker endpoint. We can use the ``sagemaker.mxnet.model.MXNetModel.deploy`` method to do this. Unless you have created or prefer other instances, we recommend using 1 ``'ml.c4.xlarge'`` instance for this training. These are supplied as arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an inference request\n",
    "\n",
    "Now that our Endpoint is deployed and we have a ``predictor`` object which we can call for inference.\n",
    "\n",
    "We wiill pass a single batch of an aligned image as a numpy array in the shape the model expects, setting `content_type` and `serializer` to convert into bytes.  The `predict.py` endpoint includes overides for `model_fn` to load fully connected layer and `transform_fn` to [transform](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html?highlight=input_fn#using-input-fn-predict-fn-and-output-fn) to load numpy input and return normalized emeddings as json.\n",
    "\n",
    "The SageMaker MXNet containers are [open source](https://github.com/aws/sagemaker-containers) if you needed more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass last aligned input into model\n",
    "input = np.expand_dims(aligned, axis=0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_bytes_serializer(data):\n",
    "    import io\n",
    "    import numpy as np\n",
    "    \n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import transform_fn\n",
    "\n",
    "data = numpy_bytes_serializer(input)\n",
    "embedding, content_type = transform_fn(model, data, 'application/x-npy', 'application/json')\n",
    "np.array(json.loads(embedding))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set the content-type to numpy \n",
    "predictor.serializer = numpy_bytes_serializer\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/x-npy'\n",
    "response = predictor.predict(input)\n",
    "print(np.array(response)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opimtize your model with Neo API\n",
    "\n",
    "Neo API allows to optimize our model for a specific hardware type. When calling `compile_model()` function, we specify the target instance family (C5) as well as the S3 bucket to which the compiled model would be stored.\n",
    "\n",
    "*Important*. If the following command result in a permission error, scroll up and locate the value of execution role returned by `get_execution_role()`. The role must have access to the S3 bucket specified in ``output_path``.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = model.data_shapes[0][1]\n",
    "output_path = '/'.join(model_data.split('/')[:-1])\n",
    "target_instance_family = 'ml_m4'  # 'rasp3b'\n",
    "compiled_model = sagemaker_model.compile(job_name=sagemaker_model.name+'-compile',\n",
    "                                   target_instance_family=target_instance_family, \n",
    "                                   input_shape={'data':input_shape},\n",
    "                                   role=role,\n",
    "                                   output_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an inference Endpoint\n",
    "\n",
    "We can deploy this compiled model, note that we need to use the same instance that the target we used for compilation. This creates a SageMaker endpoint that we can use to perform inference. \n",
    "\n",
    "The arguments to the ``deploy`` function allow us to set the number and type of instances that will be used for the Endpoint. Make sure to choose an instance for which you have compiled your model, so in our case  `ml_c5`. Neo API uses a special runtime (DLR runtime), in which our optimzed model will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_predictor = compiled_model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This endpoint will receive uncompressed NumPy arrays, whose Content-Type is given as `application/x-npy`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an inference request\n",
    "\n",
    "Now that our Endpoint is deployed and we have a ``compiled_predictor`` object which we can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Get the compiled predictor response\n",
    "compiled_predictor.accept = 'application/json'\n",
    "compiled_predictor.content_type = 'application/x-npy'\n",
    "compiled_predictor.serializer = numpy_bytes_serializer\n",
    "response = compiled_predictor.predict(input)\n",
    "print(np.array(response)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Compare this with the inline model\n",
    "local_embedding = get_feature(model, aligned)\n",
    "print(local_embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Lambda Layers\n",
    "\n",
    "### Create Model layer\n",
    "\n",
    "Copy the predict.py into `python` folder with the untared compiled model, and create a new zip file for the lambda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf python\n",
    "!mkdir -p python/compiled\n",
    "!aws s3 cp $compiled_model.model_data ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unzip the model and move to a `compiled` folder under the python directory.  \n",
    "* Rename the compiled model, and remove the shape file that isn't required.\n",
    "* Then copy the `predict.py` file and create a zip file which can be used as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tar zxvf python/*.tar.gz -C python/compiled\n",
    "mv python/compiled/compiled.params python/compiled/model.params\n",
    "mv python/compiled/compiled_model.json python/compiled/model.json\n",
    "mv python/compiled/compiled.so python/compiled/model.so\n",
    "rm python/*.tar.gz python/compiled/model-shapes.json\n",
    "cp predict.py ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the python library\n",
    "!du -h python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Layer \n",
    "\n",
    "You can download the [predict_layer.zip](./predict_layer.zip).  Create a layers for predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r predict_layer.zip ./python -x '*__pycache__*'\n",
    "!aws lambda publish-layer-version --layer-name 'predict' \\\n",
    "    --compatible-runtimes python3.6 python3.7 \\\n",
    "    --zip-file fileb://predict_layer.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local lambda Layer\n",
    "\n",
    "Download the DLR layer alongside the compiled model so that we can test inference.  \n",
    "\n",
    "This and the Pillow layer are also available via ARN:\n",
    "\n",
    "* [dlr](https://s3.amazonaws.com/deeplens-virtual-concierge-model/dlr-1.0-py3.7.zip) at ARN `arn:aws:lambda:us-east-1:882607831196:layer:dlr:4`\n",
    "* [pillow](https://s3.amazonaws.com/deeplens-virtual-concierge-model/pillow-5.4.1-py3.7.zip) at ARN: `arn:aws:lambda:us-east-1:882607831196:layer:pillow:1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the DLR runtime and unzip to the python directory for testing\n",
    "!wget -q -nc https://s3.amazonaws.com/deeplens-virtual-concierge-model/dlr-1.0-py3.7.zip\n",
    "!unzip -q -o dlr-1.0-py3.7.zip -x '*__pycache__*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relative python path for local testing\n",
    "import sys\n",
    "sys.path.append('./python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import predict\n",
    "import json\n",
    "import time\n",
    "\n",
    "dlr_model = None\n",
    "\n",
    "# Run the inference\n",
    "if dlr_model == None:\n",
    "    print('loading model')\n",
    "    dlr_model = predict.neo_load(model_path='python/compiled')\n",
    "    \n",
    "output = predict.neo_inference(dlr_model, input)\n",
    "predict.neo_postprocess(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lambda\n",
    "\n",
    "TODO: Use SAM cli to create a lambda function with layers and invoke payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat handler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an image, and the bounding box and output an event payload to send to lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load image bytes payload\n",
    "import io\n",
    "import base64\n",
    "import boto3\n",
    "\n",
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "# Read an image\n",
    "with open('tmp/image', 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "# Call rekognition to get bbox\n",
    "ret = rekognition.detect_faces(\n",
    "    Image={\n",
    "        'Bytes': payload\n",
    "    },\n",
    "    Attributes=['DEFAULT'],\n",
    ")\n",
    "\n",
    "# Create the lambda event\n",
    "event = { \n",
    "    'Image': { 'Bytes': str(base64.b64encode(payload), 'utf-8') },\n",
    "    'BoundingBox': ret['FaceDetails'][0]['BoundingBox'],\n",
    "}\n",
    "\n",
    "# Decode the payload\n",
    "payload = base64.b64decode(event['Image']['Bytes'])\n",
    "bbox = event['BoundingBox']\n",
    "image = predict.neo_preprocess(payload, 'application/x-image', bbox=bbox)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "Delete the regular and neo endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(compiled_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear all stored model data so that we don't overwrite them the next time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('tmp')\n",
    "shutil.rmtree('model')\n",
    "shutil.rmtree('python')\n",
    "!rm *.zip *.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
