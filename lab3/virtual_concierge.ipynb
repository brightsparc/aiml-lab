{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Concierge \n",
    "\n",
    "## Face Recognition Project with MXNet\n",
    "\n",
    "***\n",
    "Copyright [2017]-[2018] Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n",
    "\n",
    "http://aws.amazon.com/apache2.0/\n",
    "\n",
    "or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "#### Python package dependencies\n",
    "\n",
    "The following packages need to be installed before proceeding:\n",
    "\n",
    "* Boto3 - `pip install boto3`\n",
    "* MXNet - `pip install mxnet`\n",
    "* numpy - `pip install numpy`\n",
    "* OpenCV - `pip install opencv-python`\n",
    "* Graphviz - `pip install graphviz`\n",
    "* Matplotlib - `pip install matplotlib`\n",
    "* Seaborn - `pip install seaborn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "Verify that all dependencies are installed using the cell below. Continue if no errors encountered, warnings can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import boto3\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import os\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model\n",
    "\n",
    "`get_model()` : Loads MXNet symbols and params, defines model using symbol file and binds parameters to the model using params file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(ctx, image_size, model_str, layer):\n",
    "    model_parts = model_str.split(',')\n",
    "    assert len(model_parts)==2\n",
    "    prefix = model_parts[0]\n",
    "    epoch = int(model_parts[1])\n",
    "    print('loading',prefix, epoch)\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    all_layers = sym.get_internals()\n",
    "    sym = all_layers[layer+'_output']\n",
    "    model = mx.mod.Module(symbol=sym, context=ctx, label_names = None)\n",
    "    model.bind(data_shapes=[('data', (1, 3, image_size[0], image_size[1]))])\n",
    "    model.set_params(arg_params, aux_params)\n",
    "    return model, sym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images\n",
    "\n",
    "In order to input only face pixels into the network, all input images are passed through a pretrained face detection and alignment model as described above. The output of this model are landmark points and a bounding box corresponding to the face in the image. Using this output, the image is processed using affine transforms to generate the aligned face images which are input to the network. The functions performing this is defined below.\n",
    "\n",
    "`get_input()` : Returns aligned face to the bbox and margin, and [rotation](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html)\n",
    "\n",
    "`show_input()` : Shows the image after transposing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(img, image_size, bbox=None, rotate=0, margin=0):\n",
    "    if bbox is None:\n",
    "        det = np.zeros(4, dtype=np.int32)\n",
    "        det[0] = int(img.shape[1]*0.0625)\n",
    "        det[1] = int(img.shape[0]*0.0625)\n",
    "        det[2] = img.shape[1] - det[0]\n",
    "        det[3] = img.shape[0] - det[1]\n",
    "    else:\n",
    "        det = bbox\n",
    "    # Crop\n",
    "    bb = np.zeros(4, dtype=np.int32)\n",
    "    bb[0] = np.maximum(det[0]-margin/2, 0)\n",
    "    bb[1] = np.maximum(det[1]-margin/2, 0)\n",
    "    bb[2] = np.minimum(det[2]+margin/2, img.shape[1])\n",
    "    bb[3] = np.minimum(det[3]+margin/2, img.shape[0])\n",
    "    img = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "    # Rotate if required\n",
    "    if 0 < rotate and rotate < 360:\n",
    "        rows,cols,_ = img.shape\n",
    "        M = cv2.getRotationMatrix2D((cols/2,rows/2),360-rotate,1)\n",
    "        img = cv2.warpAffine(img,M,(cols,rows))\n",
    "    # Resize and transform\n",
    "    img = cv2.resize(img, (image_size[1], image_size[0]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aligned = np.transpose(img, (2,0,1))\n",
    "    return aligned\n",
    "\n",
    "def show_input(aligned):\n",
    "    plt.imshow(np.transpose(aligned,(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features\n",
    "\n",
    "`l2_normalize()`: Performs row normalization on the vector\n",
    "\n",
    "`get_feature()` : Performs forward pass on the data aligned using model and returns the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(X):\n",
    "    norms = np.sqrt((X * X).sum(axis=1))\n",
    "    X /= norms[:, np.newaxis]\n",
    "    return X\n",
    "\n",
    "def get_feature(model, aligned):\n",
    "    input_blob = np.expand_dims(aligned, axis=0)\n",
    "    data = mx.nd.array(input_blob)\n",
    "    db = mx.io.DataBatch(data=(data,))\n",
    "    model.forward(db, is_train=False)\n",
    "    embedding = model.get_outputs()[0].asnumpy()\n",
    "    embedding = l2_normalize(embedding).flatten()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model\n",
    "\n",
    "Load the pre-trained mobilenet mobile, setting the context to cpu and visualize the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_size = (112,112)\n",
    "model_name = './models/mobilefacenet/mobilenet1,0'\n",
    "model, sym = get_model(mx.cpu(), image_size, model_name, 'fc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.viz.plot_network(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Alignment\n",
    "\n",
    "We can use the the MTCNN algorithm to detect face bounding boxes.  \n",
    "\n",
    "Insightface MXNET version adapted from original caffe version [Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks](https://github.com/kpzhang93/MTCNN_face_detection_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn_detector import MtcnnDetector\n",
    "\n",
    "model_folder = './models/mtcnn'\n",
    "detector =  MtcnnDetector(model_folder, ctx=mx.cpu(), num_worker=1, accurate_landmark=True)\n",
    "\n",
    "def detect_bboxes(img):\n",
    "    ret = detector.detect_face(img)\n",
    "    if ret is not None:\n",
    "        bbox, points = ret\n",
    "        rotate = 0\n",
    "        return bbox.astype(int).tolist(), rotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could make use of the Rekognition library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rekognition = boto3.client('rekognition')\n",
    "\n",
    "def get_bboxes(img, margin=0):\n",
    "    # Detect faces\n",
    "    ret, buf = cv2.imencode('.jpg', img)\n",
    "    ret = rekognition.detect_faces(\n",
    "        Image={\n",
    "            'Bytes': buf.tobytes()\n",
    "        },\n",
    "        Attributes=['ALL'], # require OrientationCorrection\n",
    "    )\n",
    "    # Get the rotation\n",
    "    rotate = int(ret.get('OrientationCorrection', 'ROTATE_0').strip('ROTATE_'))\n",
    "    # Return the bounding boxes for each face\n",
    "    height, width, _ = img.shape\n",
    "    bboxes = []\n",
    "    for face in ret['FaceDetails']:\n",
    "        box = face['BoundingBox']\n",
    "        x1 = int(box['Left'] * width)\n",
    "        y1 = int(box['Top'] * height)\n",
    "        x2 = int(box['Left'] * width + box['Width'] * width)\n",
    "        y2 = int(box['Top'] * height + box['Height']  * height)\n",
    "        bboxes.append((x1, y1, x2, y2))\n",
    "    return bboxes, rotate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulate\n",
    "\n",
    "Download sample image, and extract face coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://aiml-lab-sagemaker/politicians/politicians1.jpg tmp/image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls container/local_test/Tom_Hanks_54745.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the image, and get bboxes\n",
    "img = cv2.imread('tmp/image')\n",
    "boxes, rotate = detect_bboxes(img) #get_bboxes(img)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the coordinates, get a the aligned image, and draw the rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue, green, red, grey\n",
    "colors = ((220,220,220),(242,168,73),(76,182,252),(52,194,123))\n",
    "\n",
    "img_aligned = []\n",
    "for col, bbox in enumerate(boxes): \n",
    "    img_aligned.append(get_input(img, image_size, bbox, rotate))\n",
    "    cv2.rectangle(img, (bbox[0], bbox[1]), (bbox[2], bbox[3]), colors[col], 3)\n",
    "    \n",
    "# Plot the figure in it's original rotation\n",
    "plt.figure(figsize=(10,10))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the aligned image\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i, aligned in enumerate(img_aligned):\n",
    "    a = fig.add_subplot(1, len(img_aligned), i+1)\n",
    "    a.set_title('Image {}'.format(i))\n",
    "    show_input(aligned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embedding\n",
    "\n",
    "Pass each face through the network sequentially to generate embedding vectors for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vecs = np.array([get_feature(model, aligned) for aligned in img_aligned])\n",
    "print(img_vecs.shape)\n",
    "img_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity\n",
    "\n",
    "Calculate the cosine similarity between the embedding vectors to see how similar they are to each out. \n",
    "\n",
    "Similarity values in [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.dot(img_vecs, img_vecs.T)\n",
    "np.fill_diagonal(sims, 0)\n",
    "sns.heatmap(sims, annot=True, fmt=\".03f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Dataset\n",
    "\n",
    "Download a the politician dataset, and vectories the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp/images\n",
    "!aws s3 sync s3://aiml-lab-sagemaker/actors/ tmp/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_dir = 'tmp/images'\n",
    "names = []\n",
    "vecs = []\n",
    "\n",
    "for file in os.listdir(image_dir):\n",
    "    name = file.split('.')[0]\n",
    "    img = cv2.imread(os.path.join(image_dir, file))\n",
    "    bboxes, rotate = get_bboxes(img)\n",
    "    bbox = bboxes[0]\n",
    "    print(name, bbox, rotate)\n",
    "    aligned = get_input(img, image_size, bbox, rotate)\n",
    "    vec = get_feature(model, aligned)   \n",
    "    names.append(name)\n",
    "    vecs.append(vec)\n",
    "    \n",
    "vecs = np.array(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vectors back to a file with the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('models/people.npz', names=names, vecs=vecs)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distribution\n",
    "\n",
    "Compare the vectors of all the politications to input image, plot the distribution and outliner for match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_vecs[2]\n",
    "\n",
    "# calculate cosine similarity and relative zscores\n",
    "sims = np.dot(vecs, img)\n",
    "zscores = stats.zscore(sims)\n",
    "\n",
    "# plot series and print score and name\n",
    "sns.set(color_codes=True)\n",
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.distplot(zscores, bins=50, kde=False, rug=True)\n",
    "ax.set(xlabel='zscore', ylabel='number of people')\n",
    "plt.title('zscore distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the name of the highest similarity based on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import erf, sqrt\n",
    "def phi(x):\n",
    "    #'Cumulative distribution function for the standard normal distribution'\n",
    "    return (1.0 + erf(x / sqrt(2.0))) / 2.0\n",
    "\n",
    "idx = sims.argmax()\n",
    "print('sim: {}, zscore: {}, prob: {}, name: {}'.format(sims[idx], zscores[idx], phi(zscores[idx]), names[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy custom Container\n",
    "\n",
    "We can bring our own [pre-trained mxnet model](https://aws.amazon.com/blogs/machine-learning/bring-your-own-pre-trained-mxnet-or-tensorflow-models-into-amazon-sagemaker/) so that we can form inference in sagemaker.\n",
    "\n",
    "### Create model file\n",
    "\n",
    "First step is to create a zip of the model and people artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "def flatten(tarinfo):\n",
    "    tarinfo.name = os.path.basename(tarinfo.name)\n",
    "    return tarinfo\n",
    "    \n",
    "tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n",
    "tar.add(\"./models/\") #, filter=flatten)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the sagemaker environment, getting the role and default bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model\n",
    "\n",
    "Before hosting a model, the model artifacts have to be moved to an Amazon S3 bucket After the artifacts are in the S3 bucket, the model can be deployed as an endpoint on Amazon SageMaker. To perform these tasks, we can make use of the  that accompanies Amazon SageMaker. The Python SDK conveniently provides us an API to upload our models into S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "model_data = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint\n",
    "\n",
    "1) Build the docker container similar to the BYO [sci-kit learn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/scikit_bring_your_own/container) container. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Then use [python sdk](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-deploy-model.html) to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "model_name = 'sagemaker-virtual-concierge'\n",
    "primary_container = {\n",
    "    'Image': '423079281568.dkr.ecr.ap-southeast-2.amazonaws.com/sagemaker-virtual-concierge',\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "create_model_response = sagemaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "endpoint_config_name = 'sagemaker-virtual-concierge-config-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_config_name)\n",
    "create_endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.large',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then creat the endpoint and wait until it is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'sagemaker-virtual-concierge-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "try:\n",
    "    sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Arn: \" + resp['EndpointArn'])\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "    if status != 'InService':\n",
    "        message = sagemaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Create endpoint failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "Get an image, and [send the bytes](https://medium.com/@julsimon/using-chalice-to-serve-sagemaker-predictions-a2015c02b033) to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "payload = open('container/local_test/Tom_Hanks_54745.png', 'rb').read()\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='text/csv', \n",
    "                                   Body=payload)\n",
    "\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Post image data directly\n",
    "data = open('container/local_test/Tom_Hanks_54745.png', 'rb').read()\n",
    "response = requests.post('http://localhost:8080/invocations', data=data, headers={'Content-Type': 'application/image-x'})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Wrap data in base64 encoded json so we can base a boundary box\n",
    "payload = { 'data': base64.b64encode(data).decode(\"utf-8\", \"ignore\") }\n",
    "json_data = json.dumps(payload)\n",
    "response = requests.post('http://localhost:5000/invocations', data=json_data, headers={'Content-Type': 'application/json'})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda Layers\n",
    "\n",
    "Turn this container into a lambda layere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
